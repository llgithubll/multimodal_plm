

## TODO

* https://arxiv.org/abs/2109.10246
* ViLT
* SimVLM
* MLIM


## Survey

* [å¤šæ¨¡æ€é¢„è®­ç»ƒç»¼è¿°](https://mp.weixin.qq.com/s/X-G7ARBWG49rKDQSHJ1ICQ)
* [awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)
* [vit-pytorch](https://github.com/lucidrains/vit-pytorch)
* [ä»LXMERTåˆ°VLMOï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹çš„æ¼”å˜å²](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247547810&idx=2&sn=4e3c845e049340921c5568bae1def6ce)
* [ä¸‡å­—ç»¼è¿°ï¼ä»21ç¯‡æœ€æ–°è®ºæ–‡çœ‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ç ”ç©¶è¿›å±•](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247546641&idx=1&sn=0cdc826b91e3317674222f748873d730&chksm=96eae891a19d61879deafa00790e1d38cc7b8860b1ac3da071f341b10965ac4f4bba2d46e692&token=1937743143&lang=zh_CN&scene=21#wechat_redirect)
* [ä»å¤šç¯‡2021å¹´é¡¶ä¼šè®ºæ–‡çœ‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹æœ€æ–°ç ”ç©¶è¿›å±•](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247543295&idx=1&sn=795bd6c44a425912ba3519ed63c35d95&chksm=96eaf67fa19d7f696c75f4ce1ededc7a7c54c4403bbb873b249f32ad8d8e9768dde908d0f13c&token=1937743143&lang=zh_CN&scene=21#wechat_redirect)

### Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods

* JAIR2021
* [paper](https://arxiv.org/abs/1907.09358?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)
* 135pages
* ç»¼è¿°æœ‰ç‚¹è€


### 




## Summary

| No.      | model       | Description    |
| :---  |    :----   |          :---: |
|    1   | ViT         | ...    |
|    2   | InterBERT        | ...       |
|    3   | fashionBERT      | ...       |
|    4   | Kaleido-BERT     | ...       |









(æ ‡é¢˜ä¸­åºå·ä»…è¡¨ç¤ºæ·»åŠ é¡ºåº)

### 1. AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

* ğŸŒŸ
* ICLR2021
* [paper](https://arxiv.org/abs/2010.11929)
* [code](https://github.com/google-research/vision_transformer), [vit-pytorch](https://github.com/lucidrains/vit-pytorch)
* [weixin](https://mp.weixin.qq.com/s/KyL74PwoXDWuuPLiziYPRA)

---

æŠŠå›¾ç‰‡åˆ†å‰²æˆä¸€å—ä¸€å—çš„patchï¼Œå®Œå…¨åˆ©ç”¨å’ŒBERTä¸€æ ·çš„ç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œå°†é¢„è®­ç»ƒä»æ–‡æœ¬æ‰©å±•åˆ°å›¾ç‰‡ä¸­ï¼›ï¼ˆå±äºæŒ–æ–°å‘çš„å·¥ä½œï¼‰

![vit](images/vit.png)


### 2. InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining

* KDD2020
* [paper](https://arxiv.org/abs/2003.13198)

---
* æ¨¡å‹ç»“æ„ï¼šåŒæµemb+å•ä¸€transformer+åŒæµè¾“å‡º
* è®­ç»ƒæ–¹æ³•ï¼š 
    * masked segment modeling
    *  masked region modeling 
    * image-text matching

> é˜¿é‡Œå·´å·´ã€ç”µå•†ï¼›å¯æ¨¡ä»¿å†™ä½œ

![interBERT](images/interBERT.png)

### 3. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval

* SIGIR20 Industry Track
* [paper](https://arxiv.org/abs/2005.09801)
* [link](https://developer.aliyun.com/article/763357)

---

* æå‡ºä¸€ç§å›¾æ–‡åŒ¹é…æ¨¡å‹ï¼Œä»¥æ±‚å¯¹ç”µå•†å›¾åƒç‰¹å¾æ›´å¥½çš„æå–å’Œè¡¨è¾¾
* æ¨¡å‹ç»“æ„ï¼šåŒæµæ¨¡å‹
* è®­ç»ƒæ–¹æ³•ï¼š
  * masked language modeling
  * masked patch modeling
  * text and image alignment
  * ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªadaptive lossè‡ªé€‚åº”æ¯ä¸ªä»»åŠ¡æƒé‡
  
![fashionbert](images/fashionbert.png)


### 4. Kaleido-BERT: Vision-Language Pre-training on Fashion Domain

* CVPR2021
* [paper](https://arxiv.org/abs/2103.16110)
* [ata link](https://ata.alibaba-inc.com/articles/210439?spm=ata.23639746.0.0.673a5da3nIPN0w)

---

* å¤šå°ºåº¦å›¾åƒç‰¹å¾æå–ï¼ˆKPGï¼‰
* å›¾æ–‡é¢„å¯¹é½ï¼ˆAAGã€AGMï¼‰
* å›¾åƒè‡ªç›‘ç£ï¼ˆtask: AKPM, TIM, AMLMï¼‰

![KaleidoBERT](images/kaleidoBERT.png)

> *è®ºæ–‡ä¸­çš„å®éªŒè¡¨æ ¼åšçš„ä¸é”™ï¼Œç›¸å½“äºæ¨¡å‹ç»¼è¿°äº†*

### 5. Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers

* arXiv2004
* [paper](https://arxiv.org/abs/2004.00849)

---

ä¸»è¦ç›®æ ‡åœ¨äºé€šè¿‡å›¾æ–‡åŒ¹é…ä»»åŠ¡è·å–æ›´å¥½çš„å›¾æ–‡è”åˆè¡¨ç¤ºï¼Œæ–¹æ³•è¿‡äºç®€å•

1. BERTç¼–ç æ–‡å­—ï¼ŒCNNç¼–ç å›¾ç‰‡ï¼Œåˆå¹¶æ¥ä¸Štransformerå±‚
2. è®­ç»ƒä»»åŠ¡ï¼šå›¾æ–‡åŒ¹é…ä»»åŠ¡+è¯­è¨€æ¨¡å‹çš„MLM

![Pixel-BERT](images/pixel-bert.png)


### 6. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

* arXiv1908
* [paper](https://arxiv.org/abs/1908.02265)

---

* æ¨¡å‹ç»“æ„ï¼šåŒæµæ¨¡å‹
  * æ–‡æœ¬ç›´æ¥é€šè¿‡BERTç¼–ç 
  * å›¾åƒé€šè¿‡é¢„è®­ç»ƒçš„ç›®æ ‡æ£€æµ‹ç½‘ç»œ(ResNet-101+Faster R-CNN)æŠ½å–å›¾åƒbounding boxes(æ ¹æ®ç½®ä¿¡åº¦ï¼Œæ•°é‡æ§åˆ¶åœ¨10-36)

* è®­ç»ƒæ–¹æ³•
  * maskæ–‡ï¼ˆå’Œbertä¸€æ ·ï¼‰
  * maskå›¾ï¼ˆMasked image regions have their image features zeroed out 90% of the time and are unaltered 10%ï¼‰
  * å›¾æ–‡åŒ¹é…ä»»åŠ¡linear(cat(<IMG>, <CLS>), 2)

![vilbert](images/vilbert.png)

![vilbert-train](images/vilbert_train.png)


### 7. UNITER: UNiversal Image-TExt Representation Learning

* ECCV2020
* [paper](https://arxiv.org/abs/1909.11740)
* [code](https://github.com/ChenRocks/UNITER)

---

* ä¸»è¦ç›®æ ‡å­¦ä¹ å›¾æ–‡çš„è”åˆè¡¨ç¤º
* æ¨¡å‹ç»“æ„
  * å›¾emb: R-CNN+location
  * æ–‡emb: bert
  
* è®­ç»ƒæ–¹æ³•
  * Masked Language Modeling (MLM)
  * Masked Region Modeling (MRM, with three variants)
  * Image-Text Matching (ITM)
  * Word-Region Alignment (WRA)
  * ä»4ä¸ªå›¾æ–‡æ•°æ®é›†è®­ç»ƒ(COCO, Visual Genome, Conceptual Captions, and SBU Captions)

![uniter](images/uniter.png)


### 8. LXMERT: Learning Cross-Modality Encoder Representations from Transformers

* EMNLP2019
* [paper](https://arxiv.org/abs/1908.07490)
* [code](https://github.com/airsplay/lxmert)

---

* ä¸»è¦ç›®æ ‡ï¼Œmost importantly, the alignment and relationships between these two modalities.
* æ¨¡å‹ç»“æ„
  * å›¾ç‰¹å¾ï¼Œå›¾encode
  * æ–‡ç‰¹å¾ï¼Œæ–‡encode
  * å›¾æ–‡cross encoder
  
* è®­ç»ƒæ–¹æ³•
  * masked language modeling 
  * masked object prediction (feature regression and label classiï¬cation) 
  * cross-modality matching 
  * image question answering

![lxmert](images/lxmbert.png)

![lxmert-pretrain](images/lxmbert_pretrain.png)


### 9. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

* ICML 2021
* [paper](https://arxiv.org/abs/2102.03334)
* [code](https://github.com/dandelin/vilt)

---

* ä¸»è¦ç›®æ ‡ï¼Œå°†æ–‡æœ¬ã€å›¾ç‰‡è”åˆè®­ç»ƒè½¬åˆ°BERTçš„ç»Ÿä¸€æ ¼å¼äº†ï¼›æ¨¡å‹ç®€å•ï¼Œæ•ˆç‡é«˜ï¼›
* æ¨¡å‹ç»“æ„
  * å›¾emb
  * æ–‡emb
  * å›¾æ–‡cross encoder
  
* è®­ç»ƒæ–¹æ³•
  * ITM(image text matching)ï¼ŒåŒ…å«WPA(word patch alignmentæ“ä½œ)ï¼Œè¿™é‡Œæ•ˆä»¿äº†UNITERä¸­éƒ½WRAï¼Œä½¿ç”¨çš„éƒ½æ˜¯optimal transportsæ–¹æ³•ï¼Œè®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„æœ€å°è½¬æ¢ä»£ä»·[zhihu link](https://zhuanlan.zhihu.com/p/82424946)
  * MLM(whole word masking, è¿™éƒ½ä¸ç®—å•¥åˆ›æ–°ç‚¹ï¼Œå‡‘å­—æ•°)
  * IA(image augment) during fine-tuning

>ï¼ˆæ€æƒ³åˆ°æ²¡å•¥æ–°é²œæ„Ÿã€å¤šæ¨¡æ€çš„è®ºæ–‡éƒ½æ²¡å•¥æ–°é²œæ„Ÿï¼Œè¿ç§¯æœ¨éƒ½æ‡’å¾—æ‹¼äº†ï¼‰

![vilt-d](images/ViLT_d.png)

![vilt](images/ViLT.png)

### 10. InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining

* KDD 2020
* [paper]()




### 11. Oscar:



### 12. CLIP


### 13. UNIMO


### 14. Seeing Out of tHe bOx: 

* CVPR 2021 oral
* [paper](https://arxiv.org/abs/2104.03135)
* [code](https://github.com/researchmm/soho)



### 15. VinVL

* CVPR 2021
* [paper](https://arxiv.org/abs/2101.00529)
* [code](https://github.com/pzzhang/VinVL)


### 16. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

* ICML 2021
* [paper](https://arxiv.org/abs/2102.05918)

### 17. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning

* ACL 2021
* [paper](https://arxiv.org/abs/2106.01804)


### 18. MURAL: Multimodal, Multitask Retrieval Across Languages

* EMNLP 2021
* [paper](https://arxiv.org/abs/2109.05125)



### 19. Large-Scale Adversarial Training for Vision-and-Language Representation Learning

* NeurIPS 2020


### 20. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning

* ACL 2021





