

## TODO

## Idea

### èƒ¡æ€ä¹±æƒ³1

ç»“åˆCLIPå’ŒPromptå­¦ä¹ ï¼›knowledge prompt imageï¼›

* [KnowPrompt & OntoPrompt](https://mp.weixin.qq.com/s/HPQ8AjoVN6UsHZdag1-xZw)
* [PromptPapers](https://github.com/thunlp/PromptPapers)
* [å¤šæ¨¡æ€ä¸­çš„prompt](https://mp.weixin.qq.com/s/hegK-Aut8s6KfYIe-glgag)
* [å¤šæ¨¡æ€æœ€æ–°æ–¹å‘ï¼šå„ç§é—®é¢˜ï¼Œç»†åˆ†æ–¹å‘](https://zhuanlan.zhihu.com/p/389287751)
* [åpromptæ—¶ä»£](https://mp.weixin.qq.com/s/EOJGIXSo7bj7oi0nqbbMlw)

çŸ¥è¯†æç¤ºçš„é—®é¢˜ï¼š
1. å¦‚ä½•é«˜æ•ˆåœ°æ¤å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œæ„å»ºæç¤ºæ¨¡æ¿å’Œæ ‡ç­¾æ˜ å°„ï¼Ÿ
2. å¹¶ä¸æ˜¯æ‰€æœ‰å¤–éƒ¨çŸ¥è¯†éƒ½æœ‰å¸®åŠ©ï¼Œå¦‚ä½•è§£å†³çŸ¥è¯†å™ªéŸ³å’Œå¼‚æ„æ€§é—®é¢˜ï¼Ÿ

---

* `[cls]` image `[sep]` the **itemcate** **proporty** is `?`?
* `[cls]` image `[sep]` the type of **itemcate_level1** is `?`?
* `[cls]` image `[sep]` the color of t-shirt in picture is `?(red, black, unkonwn)`

å•†å“ç†è§£ã€cpvè¿™ç§å†…å®¹ï¼Œå¯ä»¥ç”¨æ¥æ„å»ºpromptå¥å¼ï¼›å¥å¼å°±ç›´æ¥é—®éidç±»å‹ï¼ˆå“ç‰Œã€å‹å·ï¼‰çš„å±æ€§ï¼ˆé¢œè‰²ã€æ¬¾å¼ï¼‰

ä¸€é—®å“ç±»ã€äºŒé—®å±æ€§

property prompt in ecommerce multimodal pretrain

visual question answering å»ºæ¨¡ promptï¼šç»™å®šä¸€ä¸ªå›¾åƒï¼Œæ ¹æ®æ ‡é¢˜æˆ–cpvä¸­çš„å±æ€§ æ¥æ„é€ é—®é¢˜ï¼›ï¼ˆprompt or vqa)
1. é—®ä»€ä¹ˆé—®é¢˜
  * ä»€ä¹ˆé—®é¢˜æ˜¯å¯ä»¥æ ¹æ®å›¾ç‰‡å›ç­”çš„ï¼Ÿï¼ˆå“ªäº›å±æ€§ã€ç­”ä¸å‡ºæ¥æ€ä¹ˆå¤„ç†ï¼›å“ç±»å¯ä»¥é—®ï¼Œæ­»é©¬å½“æ´»é©¬åŒ»ï¼‰
  * æ€ä¹ˆå»æ„é€ æ¯”è¾ƒå¹²å‡€çš„é—®é¢˜ï¼ˆpromptï¼‰
  * é—®ï¼š
    * æ˜¯ä»€ä¹ˆ
    * å–ä»€ä¹ˆ
    * æœ‰ä»€ä¹ˆ
    * XXå±æ€§æ˜¯ä»€ä¹ˆ
    ç”¨å¦ä¸€æ¨¡æ€è¾…åŠ©
      ä»¥maskçš„æ–¹å¼å›ç­”å³å¯ï¼Ÿæˆ–è€…åˆ©ç”¨ encode-decodeæ¶æ„ï¼›å°†MAEçš„decodeæ›¿æ¢ä¸ºbertï¼Œdecodeå†…å®¹æ›¿æ¢ä¸ºç­”æ¡ˆã€‚
2. å›¾ç‰‡é—®é¢˜
  * æ·˜å®å•†å“çš„ä¸»å›¾ å’Œ çœŸå®çš„æ‹ç…§å®ç‰©å›¾ æœ‰äº›åŒºåˆ«ï¼›æ·˜å®å•†å“å›¾é€šå¸¸æ˜¯åšäº†åŠ å·¥çš„ï¼ˆæ¯”å¦‚é…ä¸Šæ–‡å­—ã€æ’ç‰ˆç­‰ï¼‰ï¼Œä¼šåŒ…å«å¾ˆå¤šè¾…åŠ©ä¿¡æ¯
  * ç›¸å½“å¤šï¼ˆç›¸å½“å¤šï¼‰çš„å›¾ç‰‡ä¸ŠåŒ…å«æ–‡æœ¬ä¿¡æ¯ï¼ˆé¡ºä¸°åŒ…é‚®ã€å“ç‰Œåï¼Œå·´æ‹‰å·´æ‹‰ï¼‰
  * ä¸»ä½“è¯†åˆ«ï¼ˆåŠ å¤§ä¸»ä½“maskæ¦‚ç‡ï¼šç•™çŸ³ï¼‰ï¼Œå¿½ç•¥èƒŒæ™¯ã€æ’ç‰ˆã€æ–‡å­—çš„å½±å“ï¼Œå•æµæ¨¡å‹ or å…¶ä»–
  * 


---

é˜¶æ®µ1: ä¸‰ç»„å¯¹æ¯”ï¼Œ Query-Title, Query-Image, Title-Image
é˜¶æ®µ2:
* prompt åŒæµ äº¤äº’ï¼›ç­”æ¡ˆç›´æ¥maskå°±ok
* prompt å•æµï¼Œå›¾ç‰‡ç”¨VITï¼Œé—®é¢˜ç”¨BERTï¼Œç»“æ„æ˜¯encode-decodeï¼ŒMAEçš„decodeæ›¿æ¢ä¸ºbertï¼Œä¸”ç”¨æ¥å›ç­”é—®é¢˜


* [é˜¿é‡Œå°èœœVQA](https://ata.alibaba-inc.com/articles/173749?spm=ata.23639746.0.0.3a843565f75afS)


### èƒ¡æ€ä¹±æƒ³2

ç°åœ¨å¤šæ¨¡æ€è®­ç»ƒçš„ä»»åŠ¡éƒ½æ˜¯åˆçº§çš„ä»»åŠ¡ï¼›æ¯”å¦‚ image-text match, image-text contrastive; å¹¶åŒ…å«å•æ¨¡æ€çš„ç»å…¸å­¦ä¹ ä»»åŠ¡ï¼Œæ¯”å¦‚ mlm, mimç­‰ï¼›

å°±å­¦ä¹ è€Œè¨€ï¼Œåˆ¤åˆ«ä»»åŠ¡æ›´ç›´æ¥ï¼Œä¹Ÿç›¸å¯¹å®¹æ˜“ï¼›ä¸ºäº†è®©æ¨¡å‹è·å–æ›´é«˜high levelçš„ç†è§£èƒ½åŠ›ï¼›é€šè¿‡ç”Ÿæˆçš„æ–¹å¼ï¼Œè®©æ¨¡æ€ä¹‹é—´äº’ç›¸æç¤ºã€‚

text2image (VAE, GAN) 

image caption([image captioning](https://proceedings.neurips.cc/paper/2019/file/680390c55bbd9ce416d1d69a9ab4760d-Paper.pdf))

ä¸»è¦é—®é¢˜
1. ä¸¤ä¸ªç”Ÿæˆä»»åŠ¡åº”è¯¥éƒ½éå¸¸hard
2. æ·˜å®å•†å“ä¸­çš„ä¸»å›¾å’Œæ ‡é¢˜ï¼Œå¹¶éæ‰€è§å³æ‰€è¯´çš„å…³ç³»ï¼Œå­˜åœ¨ä¸€å®šæ¯”ä¾‹è¾…åŠ©æˆ–ä¸ç›¸å…³çš„ä¿¡æ¯ï¼›


### èƒ¡æ€ä¹±æƒ³3


å›¾æ–‡è”åˆè®­ç»ƒï¼ŒåŠ å…¥æ¨ç† Video-Language Inference

å¯¹äºä¸€ä¸ªå•†å“ã€‚image æ ¹æ® æ ‡é¢˜ã€cpvå†…å®¹æ¨ç†å›¾ç‰‡å†… å…³ç³»ã€å®ä½“ï¼› æ ‡é¢˜ã€cpvæ ¹æ® image å†…å®¹ æ¨ç†imageä¸­ç‰©ä½“çš„å…³ç³»ï¼ˆVCRï¼‰ æœ€åå¾—åˆ°è”åˆè¡¨ç¤ºï¼ˆå¯¹é½æ¨¡å‹ï¼‰

ä¸»è¦é—®é¢˜ï¼š
1. å›¾ç‰‡ä¸­å¯èƒ½ä¸å­˜åœ¨å…³ç³» æˆ– å…³ç³»å¤ªéš¾ç•Œå®š
2. å›¾ç‰‡ä¸­æè¿°ä¸å±•ç¤ºæ€§çš„å†…å®¹å±…å¤šï¼Œå…³ç³»å†…å®¹å¾ˆå°‘ 


### èƒ¡æ€ä¹±æƒ³4

ï¼ˆé­”æ”¹æ¨¡å‹ï¼Œæ¨¡æ€èåˆï¼Œtodo:3.26ï¼‰

![self-attention(li-hong-yi)](images/self_attention.png)

å†æ‰§è¡Œtransformerså±‚æ—¶ï¼Œå•æµæ¨¡å‹ï¼ˆæ¯”å¦‚ fashionbertï¼‰å›¾æ–‡ä¸¤ç§æ¨¡æ€ä¼šæ— å·®åˆ«çš„è¿›è¡Œself-attentionï¼›
å³å›¾ä¸­çš„patch å’Œ æ–‡ä¸­çš„ wordï¼›ä»–ä»¬é€šè¿‡ç›¸åŒçš„çº¿æ€§å±‚æ˜ å°„åˆ° q, k, v; ç„¶åç»†åˆ†å¤šä»½head(q, k, v)ï¼›
æ­¤æ—¶çš„q, k, vå°†è¿›è¡Œself-attentionæ“ä½œï¼Œç»™å®šå…ƒç´ çš„qä¼šå’Œåºåˆ—ä¸­å…¶ä»–çš„æ‰€æœ‰kè¿›è¡Œç›¸å…³æ€§è®¡ç®—ï¼ˆç‚¹ç§¯ï¼‰ï¼Œ
è¿™æ ·ä¼šå¯¼è‡´ dot(q(w1),k(w2)) & dot(q(w1),k(i1)) & dot(q(i2),k(i1)) å¹¶å­˜ï¼Œ
å³ w1 å’Œ w2, i1 éƒ½è¦è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼›å¹¶ä»¥æ­¤è¡¡é‡ w1 å’Œ w2ã€i1 çš„ç›¸ä¼¼åº¦ï¼›ä½†w å’Œ i æœ¬æ˜¯ä¸¤ç§ä¸åŒçš„æ¨¡æ€ï¼Œè¿™ç§è®¡ç®—ä¼šä½¿å¾—ä¸¤ç§æ¨¡æ€æ··åˆï¼ˆä¸æ˜¯èåˆï¼Œæ²¡æœ‰åŒºåˆ†ç‰¹ç‚¹ï¼‰ï¼ˆåŒç†, i1 å’Œ w1, i2ï¼Œè¢«è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼‰
ï¼ˆåŒºåˆ†å›¾æ–‡çš„å”¯ä¸€ä¿¡æ¯æ¥æºæ˜¯ åº•å±‚è¾“å…¥çš„token_type_idsï¼Œ **wå’Œiçš„è¡¨ç¤ºä¸å®Œå…¨å…¼å®¹ï¼Œç›´æ¥ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¾—åˆ°çš„ä¿¡æ¯(æƒé‡)ä¸å¤Ÿç²¾ç¡®**ï¼‰

*æ–°æ–¹æ³•ï¼Œwå’Œwä¹‹é—´ä½¿ç”¨æ­£å¸¸self-attentionäº¤äº’ï¼Œwå’Œiä¹‹é—´é€šè¿‡æ˜ å°„å…³ç³»hå˜æ¢åï¼Œè¿›è¡Œæ­£å¸¸self-attentionï¼šw+h->w', w'ä¸äº¤äº’ï¼›åŒç†i-h=i' i'ä¸wè¿›è¡Œäº¤äº’*
ï¼ˆtranEçš„æ€æƒ³ç›´è§‚ä¸Šè¿œæ¯”transDåˆé€‚)

![multi-head attention(li-hong-yi)](images/multi-head_attention.png)

ï¼ˆæ–°å‘å•Šï¼Œæ¨¡å‹æ¨¡æ€èåˆï¼›å‚è€ƒä¸‹blip: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generationï¼‰

---



## Survey

* [å¤šæ¨¡æ€é¢„è®­ç»ƒç»¼è¿°](https://mp.weixin.qq.com/s/X-G7ARBWG49rKDQSHJ1ICQ)
* [awesome-vision-language-pretraining-papers](https://github.com/yuewang-cuhk/awesome-vision-language-pretraining-papers)
* [vit-pytorch](https://github.com/lucidrains/vit-pytorch)
* [ä»LXMERTåˆ°VLMOï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹çš„æ¼”å˜å²](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247547810&idx=2&sn=4e3c845e049340921c5568bae1def6ce)
* [ä¸‡å­—ç»¼è¿°ï¼ä»21ç¯‡æœ€æ–°è®ºæ–‡çœ‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ç ”ç©¶è¿›å±•](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247546641&idx=1&sn=0cdc826b91e3317674222f748873d730&chksm=96eae891a19d61879deafa00790e1d38cc7b8860b1ac3da071f341b10965ac4f4bba2d46e692&token=1937743143&lang=zh_CN&scene=21#wechat_redirect)
* [ä»å¤šç¯‡2021å¹´é¡¶ä¼šè®ºæ–‡çœ‹å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹æœ€æ–°ç ”ç©¶è¿›å±•](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247543295&idx=1&sn=795bd6c44a425912ba3519ed63c35d95&chksm=96eaf67fa19d7f696c75f4ce1ededc7a7c54c4403bbb873b249f32ad8d8e9768dde908d0f13c&token=1937743143&lang=zh_CN&scene=21#wechat_redirect)
* [ğŸŒŸawesome-multimodal-ml](https://github.com/pliang279/awesome-multimodal-ml)


### Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods

* JAIR 2021
* [paper](https://arxiv.org/abs/1907.09358?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%253A+arxiv%252FQSXk+%2528ExcitingAds%2521+cs+updates+on+arXiv.org%2529)
* 135pages
* ç»¼è¿°æœ‰ç‚¹è€



## Summary

| No.      | model       | Description    |
| :---  |    :----   |          :---: |
|    1   | ViT         | ...    |
|    2   | InterBERT        | ...       |
|    3   | fashionBERT      | ...       |
|    4   | Kaleido-BERT     | ...       |









(æ ‡é¢˜ä¸­åºå·ä»…è¡¨ç¤ºæ·»åŠ é¡ºåº)

### 1. AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

* ğŸŒŸ
* ICLR 2021
* [paper](https://arxiv.org/abs/2010.11929)
* [code](https://github.com/google-research/vision_transformer), [vit-pytorch](https://github.com/lucidrains/vit-pytorch)
* [weixin](https://mp.weixin.qq.com/s/KyL74PwoXDWuuPLiziYPRA)

---

æŠŠå›¾ç‰‡åˆ†å‰²æˆä¸€å—ä¸€å—çš„patchï¼Œå®Œå…¨åˆ©ç”¨å’ŒBERTä¸€æ ·çš„ç»“æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œå°†é¢„è®­ç»ƒä»æ–‡æœ¬æ‰©å±•åˆ°å›¾ç‰‡ä¸­ï¼›ï¼ˆå±äºæŒ–æ–°å‘çš„å·¥ä½œï¼‰

![vit](images/vit.png)


### 2. InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining

* KDD 2020
* [paper](https://arxiv.org/abs/2003.13198)


---

å¤šæ¨¡æ€é¢„è®­ç»ƒã€M6ä¸­çš„å¤šæ¨¡æ€ï¼ˆ**MultiModality**-toMultiModality Multitask Mega-transformerï¼‰

* ä¸»è¦ç›®æ ‡ï¼Œå›¾æ–‡è¡¨ç¤ºï¼Œthe improvement in multi-modal representation learning
* æ¨¡å‹ç»“æ„
  * å›¾emb
  * æ–‡emb
  * å›¾æ–‡transformer
  * å›¾encoder
  * æ–‡encoder
  
* è®­ç»ƒæ–¹æ³•
  * MSM, masked segment modeling, textä¸­çš„span mask
  * MRM, masked region modeling, å›¾åƒä¸­çš„region(ç›®æ ‡æ£€æµ‹ç»“æœ,IoUs) expand mask
  * ITM, image-text matching, hard negatives: æ–‡æœ¬æè¿°tf-idfå°äº0.5çš„top30
  

> é˜¿é‡Œå·´å·´ã€ç”µå•†ï¼›å¯æ¨¡ä»¿å†™ä½œ

![interBERT](images/interBERT.png)

### 3. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval

* SIGIR 20 Industry Track
* [paper](https://arxiv.org/abs/2005.09801)
* [link](https://developer.aliyun.com/article/763357)

---

* æå‡ºä¸€ç§å›¾æ–‡åŒ¹é…æ¨¡å‹ï¼Œä»¥æ±‚å¯¹ç”µå•†å›¾åƒç‰¹å¾æ›´å¥½çš„æå–å’Œè¡¨è¾¾
* æ¨¡å‹ç»“æ„ï¼šåŒæµæ¨¡å‹
* è®­ç»ƒæ–¹æ³•ï¼š
  * masked language modeling
  * masked patch modeling
  * text and image alignment
  * ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªadaptive lossè‡ªé€‚åº”æ¯ä¸ªä»»åŠ¡æƒé‡
  
![fashionbert](images/fashionbert.png)


### 4. Kaleido-BERT: Vision-Language Pre-training on Fashion Domain

* CVPR2021
* [paper](https://arxiv.org/abs/2103.16110)
* [ata link](https://ata.alibaba-inc.com/articles/210439?spm=ata.23639746.0.0.673a5da3nIPN0w)

---

* å¤šå°ºåº¦å›¾åƒç‰¹å¾æå–ï¼ˆKPGï¼‰
* å›¾æ–‡é¢„å¯¹é½ï¼ˆAAGã€AGMï¼‰
* å›¾åƒè‡ªç›‘ç£ï¼ˆtask: AKPM, TIM, AMLMï¼‰

![KaleidoBERT](images/kaleidoBERT.png)

> è®ºæ–‡ä¸­çš„å®éªŒè¡¨æ ¼åšçš„ä¸é”™ï¼Œç›¸å½“äºæ¨¡å‹ç»¼è¿°äº†

### 5. Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers

* arXiv2004
* [paper](https://arxiv.org/abs/2004.00849)

---

ä¸»è¦ç›®æ ‡åœ¨äºé€šè¿‡å›¾æ–‡åŒ¹é…ä»»åŠ¡è·å–æ›´å¥½çš„å›¾æ–‡è”åˆè¡¨ç¤ºï¼Œæ–¹æ³•è¿‡äºç®€å•

1. BERTç¼–ç æ–‡å­—ï¼ŒCNNç¼–ç å›¾ç‰‡ï¼Œåˆå¹¶æ¥ä¸Štransformerå±‚
2. è®­ç»ƒä»»åŠ¡ï¼šå›¾æ–‡åŒ¹é…ä»»åŠ¡+è¯­è¨€æ¨¡å‹çš„MLM

![Pixel-BERT](images/pixel-bert.png)


### 6. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

* arXiv 1908
* [paper](https://arxiv.org/abs/1908.02265)

---

* æ¨¡å‹ç»“æ„ï¼šåŒæµæ¨¡å‹
  * æ–‡æœ¬ç›´æ¥é€šè¿‡BERTç¼–ç 
  * å›¾åƒé€šè¿‡é¢„è®­ç»ƒçš„ç›®æ ‡æ£€æµ‹ç½‘ç»œ(ResNet-101+Faster R-CNN)æŠ½å–å›¾åƒbounding boxes(æ ¹æ®ç½®ä¿¡åº¦ï¼Œæ•°é‡æ§åˆ¶åœ¨10-36)

* è®­ç»ƒæ–¹æ³•
  * maskæ–‡ï¼ˆå’Œbertä¸€æ ·ï¼‰
  * maskå›¾ï¼ˆMasked image regions have their image features zeroed out 90% of the time and are unaltered 10%ï¼‰
  * å›¾æ–‡åŒ¹é…ä»»åŠ¡linear(cat(<IMG>, <CLS>), 2)

![vilbert](images/vilbert.png)

![vilbert-train](images/vilbert_train.png)


### 7. UNITER: UNiversal Image-TExt Representation Learning

* ECCV 2020
* [paper](https://arxiv.org/abs/1909.11740)
* [code](https://github.com/ChenRocks/UNITER)

---

* ä¸»è¦ç›®æ ‡å­¦ä¹ å›¾æ–‡çš„è”åˆè¡¨ç¤º
* æ¨¡å‹ç»“æ„
  * å›¾emb: R-CNN+location
  * æ–‡emb: bert
  
* è®­ç»ƒæ–¹æ³•
  * Masked Language Modeling (MLM)
  * Masked Region Modeling (MRM, with three variants)
  * Image-Text Matching (ITM)
  * Word-Region Alignment (WRA)
  * ä»4ä¸ªå›¾æ–‡æ•°æ®é›†è®­ç»ƒ(COCO, Visual Genome, Conceptual Captions, and SBU Captions)

![uniter](images/uniter.png)


### 8. LXMERT: Learning Cross-Modality Encoder Representations from Transformers

* EMNLP 2019
* [paper](https://arxiv.org/abs/1908.07490)
* [code](https://github.com/airsplay/lxmert)

---

* ä¸»è¦ç›®æ ‡ï¼Œmost importantly, the alignment and relationships between these two modalities.
* æ¨¡å‹ç»“æ„
  * å›¾ç‰¹å¾ï¼Œå›¾encode
  * æ–‡ç‰¹å¾ï¼Œæ–‡encode
  * å›¾æ–‡cross encoder
  
* è®­ç»ƒæ–¹æ³•
  * masked language modeling 
  * masked object prediction (feature regression and label classiï¬cation) 
  * cross-modality matching 
  * image question answering

![lxmert](images/lxmbert.png)

![lxmert-pretrain](images/lxmbert_pretrain.png)


### 9. ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

* ICML 2021
* [paper](https://arxiv.org/abs/2102.03334)
* [code](https://github.com/dandelin/vilt)

---

* ä¸»è¦ç›®æ ‡ï¼Œå°†æ–‡æœ¬ã€å›¾ç‰‡è”åˆè®­ç»ƒè½¬åˆ°BERTçš„ç»Ÿä¸€æ ¼å¼äº†ï¼›æ¨¡å‹ç®€å•ï¼Œæ•ˆç‡é«˜ï¼›
* æ¨¡å‹ç»“æ„
  * å›¾emb
  * æ–‡emb
  * å›¾æ–‡cross encoder
  
* è®­ç»ƒæ–¹æ³•
  * ITM(image text matching)ï¼ŒåŒ…å«WPA(word patch alignmentæ“ä½œ)ï¼Œè¿™é‡Œæ•ˆä»¿äº†UNITERä¸­éƒ½WRAï¼Œä½¿ç”¨çš„éƒ½æ˜¯optimal transportsæ–¹æ³•ï¼Œè®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„æœ€å°è½¬æ¢ä»£ä»·[zhihu link](https://zhuanlan.zhihu.com/p/82424946)
  * MLM(whole word masking, è¿™éƒ½ä¸ç®—å•¥åˆ›æ–°ç‚¹ï¼Œå‡‘å­—æ•°)
  * IA(image augment) during fine-tuning

>ï¼ˆæ€æƒ³åˆ°æ²¡å•¥æ–°é²œæ„Ÿã€å¤šæ¨¡æ€çš„è®ºæ–‡éƒ½æ²¡å•¥æ–°é²œæ„Ÿï¼Œè¿ç§¯æœ¨éƒ½æ‡’å¾—æ‹¼äº†ï¼‰

![vilt-d](images/ViLT_d.png)

![vilt](images/ViLT.png)

### 10. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning

* ACL 2021
* [paper](https://arxiv.org/abs/2012.15409)
* [code](https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO)

---

**åŠ¨æœº**

å½“å‰çš„é¢„è®­ç»ƒä»»åŠ¡æˆ–åœ¨å•æ¨¡æ€ï¼Œæˆ–åœ¨å¤šæ¨¡æ€ï¼›è¿™äº›æ¨¡å‹çš„åº”ç”¨åªèƒ½å±€é™äº å•æ¨¡æ€æˆ–ç‰¹å®šçš„å¤šæ¨¡æ€ï¼ˆimage-text pairsï¼‰ï¼›
æœ¬æ–‡è®¾è®¡ä¸€ç§ unified-modalï¼Œåˆ©ç”¨å¤§é‡å›¾åƒæ–‡æœ¬ï¼ˆä½†éimage-text pairï¼‰è¿›è¡Œè·¨æ¨¡æ€å­¦ä¹ ï¼Œå°†æ–‡æœ¬å’Œå›¾åƒç¼–ç åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­ï¼›
ä½¿å¾—æ¨¡å‹æœ¬èº«ä¸ä»…èƒ½å¤Ÿå•ç‹¬å¤„ç†å›¾ã€æ–‡æ¨¡æ€ï¼Œä¹Ÿèƒ½å¤„ç†å›¾æ–‡çš„è”åˆæ¨¡æ€ã€‚

---

* ä¸»è¦ç›®æ ‡ï¼ŒæŠŠå›¾æ–‡å­¦ä¹ åˆ°åŒä¸€è¯­ä¹‰ç©ºé—´ä¸­
* è®­ç»ƒæ–¹æ³•
  * è”åˆè¡¨ç¤ºçš„ä¸åŒæ¨¡æ€è¡¨ç¤ºå¯¹æ¯”å­¦ä¹ 
  * å›¾å•æ¨¡æ€å’Œè”åˆè¡¨ç¤ºå†…çš„æ–‡æ¨¡æ€å¯¹æ¯”å­¦ä¹ 
  * æ–‡å•æ¨¡æ€å’Œè”åˆè¡¨ç¤ºå†…çš„å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ 
  
> ä¹‹å‰è€ƒè™‘è¿‡è¿™ç§åšæ³•ï¼Œæˆ–å¾ˆç±»ä¼¼

![unimo](images/unimo.png)



### 11. Oscar:



### 12. CLIP

* ğŸŒŸ
* ICML 2021
* [paper](https://arxiv.org/abs/2103.00020)
* [code](https://github.com/openai/CLIP)
* [openai blog](https://openai.com/blog/clip/)
* [bilibili video](https://www.bilibili.com/video/BV1SL4y1s7LQ)
* [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)


---

**åŠ¨æœº**

å½“å‰STOAçš„CVç³»ç»Ÿé€šå¸¸éƒ½æ˜¯åœ¨å­¦ä¹ /é¢„æµ‹ ç¡®å®šç±»åˆ«çš„ç›®æ ‡ï¼ˆe.g. åˆ©ç”¨ImageNetï¼‰ï¼Œè¿™é™åˆ¶äº†CVç³»ç»Ÿçš„æ³›åŒ–å’Œåº”ç”¨ï¼›ç›´æ¥ä¸è‡ªç„¶è¯­è¨€ä¸€èµ·å­¦ä¹ å›¾åƒè¡¨ç¤ºï¼Œ
èƒ½å¤Ÿè·å¾—æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ï¼Œä»è€Œå¯èƒ½å­¦åˆ°æ›´å¥½çš„å›¾åƒè¡¨ç¤ºã€‚æœ¬æ–‡æ”¶é›†4äº¿(image, text) pairsä½œä¸ºæ•°æ®é›†ï¼Œå°†è‡ªç„¶è¯­è¨€ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ +å›¾æ–‡åŒ¹é…ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚

CLIPè¿ç§»å­¦ä¹ æ•ˆæœéå¸¸å¥½ï¼Œé¢„è®­ç»ƒå¥½çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»»æ„è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—å¥½çš„æ•ˆæœï¼Œå¹¶ä¸”æ˜¯zero-shotçš„

---

* é¢„è®­ç»ƒç›®æ ‡ï¼šæ¯”è¾ƒæ ‡å‡†çš„ç”¨å¯¹æ¯”å­¦ä¹ åšå›¾æ–‡åŒ¹é…
* zero-shotåˆ†ç±»è¿‡ç¨‹ï¼š
  * promptå°†ç±»åˆ«æ„æˆæ–‡æœ¬ï¼Œç”Ÿæˆè¡¨ç¤º
  * å’Œå›¾ç‰‡åšç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¹¶é€šè¿‡softmaxå¾—åˆ°åˆ†ç±»

![clip](images/clip.png)

```python 
# image_encoder   - ResNet or Vision Transformer 
# text_encoder    - CBOW or Text Transformer 
# I[n, h, w, c]   - minibatch of aligned images 
# T[n, l]         - minibatch of aligned texts 
# W_i[d_i, d_e]   - learned proj of image to embed 
# W_t[d_t, d_e]   - learned proj of text to embed 
# t               - learned temperature parameter

# extract feature representations of each modality 
I_f = image_encoder(I) #[n, d_i] 
T_f = text_encoder(T) #[n, d_t]

# joint multimodal embedding [n, d_e] 
I_e = l2_normalize(np.dot(I_f, W_i), axis=1) 
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)

# scaled pairwise cosine similarities [n, n] 
logits = np.dot(I_e, T_e.T) * np.exp(t)

# symmetric loss function 
labels = np.arange(n) 
loss_i = cross_entropy_loss(logits, labels, axis=0) 
loss_t = cross_entropy_loss(logits, labels, axis=1) 
loss = (loss_i + loss_t)/2
```

### 13. Seeing Out of tHe bOx: 

* CVPR 2021 oral
* [paper](https://arxiv.org/abs/2104.03135)
* [code](https://github.com/researchmm/soho)



### 14. VinVL

* CVPR 2021
* [paper](https://arxiv.org/abs/2101.00529)
* [code](https://github.com/pzzhang/VinVL)


### 15. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

* ICML 2021
* [paper](https://arxiv.org/abs/2102.05918)

---

ALIGN

### 16. E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning

* ACL 2021
* [paper](https://arxiv.org/abs/2106.01804)


### 17. MURAL: Multimodal, Multitask Retrieval Across Languages

* EMNLP 2021
* [paper](https://arxiv.org/abs/2109.05125)

ç›¸å…³paper *Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision*

---


**åŠ¨æœº**

image-caption pairså’Œtranslation pairså¯¹äºè·¨è¯­è¨€æ·±åº¦å­¦ä¹ è‡³å…³é‡è¦

---

* è®­ç»ƒæ–¹æ³•
  * image-text å›¾æ–‡è¡¨ç¤ºå¯¹æ¯”å­¦ä¹ 
  * text-text  æ–‡æ–‡è¡¨ç¤ºï¼ˆä¸åŒè¯­ç§ï¼‰å¯¹æ¯”å­¦ä¹ 
  
![mural](images/mural.png)




### 18. Large-Scale Adversarial Training for Vision-and-Language Representation Learning

* NeurIPS 2020
* [paper](https://arxiv.org/abs/2006.06195)
* [code](https://github.com/zhegan27/LXMERT-AdvTrain)

ç›¸å…³paperï¼Œuniterã€LXMERT




### 19. SimVLM: Simple Visual Language Model Pretraining with Weak Supervision


* [paper](https://arxiv.org/abs/2108.10904)
* 


### 20. MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling

* [paper]()


### 21. Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation

* [paper](https://arxiv.org/abs/2112.05587)

---

![UniVL](images/UniVL.png)


### 22. CLIP-Event: Connecting Text and Images with Event Structures

* [paper](https://arxiv.org/abs/2201.05078)
* [code](https://github.com/limanling/clip-event)

---

**åŠ¨æœº**

ä»¥å¾€çš„V+Læ–¹æ³•ï¼Œé€šè¿‡å¯¹é½image, textå­¦ä¹ åˆ°å¤šæ¨¡æ€è¡¨ç¤ºã€‚å¹¶ä¸»è¦å…³æ³¨å›¾åƒç›®æ ‡ä¸æ–‡æœ¬ä¸­å®ä½“ä¹‹é—´çš„å¯¹é½ï¼Œæ²¡æœ‰å…³æ³¨eventsã€å‚ä¸è€…ä¹‹é—´çš„å…³ç³»ï¼ˆç²¾ç»†ç†è§£ï¼‰

---

* è®­ç»ƒæ–¹æ³•
  * å¯¹æ¯”å­¦ä¹ +prompt
  * 

---

## MISC & Other

### Does Vision-and-Language Pretraining Improve Lexical Grounding?

* EMNLP 2021
* [paper](https://arxiv.org/abs/2109.10246)


### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

* [paper](https://arxiv.org/abs/2201.12086)
* [code](https://github.com/salesforce/BLIP)

---
**åŠ¨æœº** ï¼Ÿæ²¡æœ‰å»è§£å†³ç‰¹å®šçš„é—®é¢˜ï¼›ä½†æ˜¯æ¢äº†ä¸€ç§æ–¹å¼ï¼Œå°†å¤šæ¨¡æ€çš„æ¨¡å‹èåˆäº†ï¼ˆåŸæ¥è¦ä¹ˆæ˜¯å•å¡”ã€è¦ä¹ˆæ˜¯åŒå¡”ï¼‰
æœ¬æ–‡æ˜¯å›¾åƒencoderæ¥ä¸Šæ–‡æœ¬encoderï¼Œå¹¶åœ¨æ–‡æœ¬encoderä¸­é›†æˆäº†cross-attentionï¼Œèåˆä¸¤ç§æ¨¡æ€å†…å®¹ï¼›ï¼ˆæ¨¡å‹å±‚é¢çš„èåˆï¼‰


---

* è®­ç»ƒæ–¹æ³•
  * æ¨¡å‹çº§è”çš„æ–¹å¼
    * part1 å…ˆä½¿ç”¨image encoderå¯¹å›¾ç‰‡ç¼–ç 
    * part2 å†ç”¨text encoderå¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    * part1 å’Œ part2 ä¹‹é—´ ä½¿ç”¨ITC(image-text contrastive loss)è®­ç»ƒ
    * part1 å’Œ part2 è¿›è¡Œèåˆï¼Œå¤šäº†ä¸ª cross-attention æ¨¡å—ï¼Œç”¨æ¥å°†å›¾æ–‡æ¨¡æ€è¡¨ç¤ºåˆå¹¶ï¼›
      * ä½¿ç”¨å›¾æ–‡åŒ¹é…ä»»åŠ¡è¿›è¡Œè®­ç»ƒ (image-text matching loss)
      * ä½¿ç”¨è¯­è¨€æ¨¡å‹(language modeling loss)ä»»åŠ¡æ¥é€‚é…æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ÃŸ

![BLIP gif](images/BLIP.gif)

![BLIP](images/BLIP.png)

---








